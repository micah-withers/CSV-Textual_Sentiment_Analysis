{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT_parrotmob","provenance":[{"file_id":"19ltEkQHc1vaQDMzkdCY5T-pnKQKDOhPv","timestamp":1603818776472}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"1tadN4hSCP9p"},"source":["# Stage 1: Importing dependencies"]},{"cell_type":"code","metadata":{"id":"-smF-9ymDC-Z"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EUU4TlmoFMZ_"},"source":["import numpy as np\n","import math\n","import re\n","import pandas as pd\n","from bs4 import BeautifulSoup\n","import random\n","\n","from google.colab import drive"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lXj8lk3uGn4P"},"source":["!pip install bert-for-tf2\n","!pip install sentencepiece"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fOfuPdFHFpfC"},"source":["try:\n","    %tensorflow_version 2.x\n","except Exception:\n","    pass\n","import tensorflow as tf\n","\n","import tensorflow_hub as hub\n","\n","from tensorflow.keras import layers\n","import bert"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j6ZbE2lPDIFL"},"source":["# Stage 2: Data preprocessing"]},{"cell_type":"markdown","metadata":{"id":"T9S77lewDNE1"},"source":["## Loading files"]},{"cell_type":"markdown","metadata":{"id":"J7GET0xsDSDc"},"source":["We import files from our personal Google drive."]},{"cell_type":"code","metadata":{"id":"5hABc0h8GdTe"},"source":["drive.mount(\"/content/drive\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"slnILsqwGxTX"},"source":["# cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n","data = pd.read_csv(\n","    \"/content/drive/My Drive/BERT/parrotmob_sentiment/sentiment_data.csv\",\n","    header=0,\n","    sep=',',\n","    engine=\"python\",\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"REdK4z4YG9kZ"},"source":["messages = data['message']\n","binary = data['binary_sentiment']\n","sentiment = data['sentiment']\n","respond = data['respond']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lz2g61evDZb4"},"source":["## Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"LCyy4babDrI8"},"source":["### Cleaning"]},{"cell_type":"code","metadata":{"id":"UEyorQS_HArn"},"source":["def clean_msg(msg):\n","    msg = BeautifulSoup(msg, \"lxml\").get_text()\n","    # Removing the @\n","    msg = re.sub(r\"@[A-Za-z0-9]+\", ' ', msg)\n","    # Removing the URL links\n","    msg = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', msg)\n","    # Keeping only letters\n","    msg = re.sub(r\"[^a-zA-Z.!?']\", ' ', msg)\n","    # Removing additional whitespaces\n","    msg = re.sub(r\" +\", ' ', msg)\n","    return msg"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3BlbZpy0HHiV"},"source":["# data_clean = [clean_msg(msg) for msg in messages]\n","data_clean = [msg for msg in messages]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H6SOj46BHKEk"},"source":["data_labels = [val for val in binary]\n","sent_labels = [val for val in sentiment]\n","resp_labels = [val for val in respond]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MJa3YWeJD1gM"},"source":["### Tokenization"]},{"cell_type":"markdown","metadata":{"id":"MUaCPqqBD7kQ"},"source":["We need to create a BERT layer to have access to meta data for the tokenizer (like vocab size)."]},{"cell_type":"code","metadata":{"id":"0wry-st-HMN0"},"source":["FullTokenizer = bert.bert_tokenization.FullTokenizer\n","bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n","                            trainable=False)\n","vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n","do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n","tokenizer = FullTokenizer(vocab_file, do_lower_case)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HMVarTJpELyK"},"source":["We only use the first sentence for BERT inputs so we add the CLS token at the beginning and the SEP token at the end of each sentence."]},{"cell_type":"code","metadata":{"id":"v-JkZt9NduoC"},"source":["def encode_sentence(sent):\n","    return [\"[CLS]\"] + tokenizer.tokenize(sent) + [\"[SEP]\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pel_Uk6Ic4xB"},"source":["data_inputs = [encode_sentence(sentence) for sentence in data_clean]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z32MeEwnkCB8"},"source":["### Dataset creation"]},{"cell_type":"markdown","metadata":{"id":"cUVc83VNEcW9"},"source":["We need to create the 3 different inputs for each sentence."]},{"cell_type":"code","metadata":{"id":"wmW9JZLJaxww"},"source":["def get_ids(tokens):\n","    return tokenizer.convert_tokens_to_ids(tokens)\n","\n","def get_mask(tokens):\n","    return np.char.not_equal(tokens, \"[PAD]\").astype(int)\n","\n","def get_segments(tokens):\n","    seg_ids = []\n","    current_seg_id = 0\n","    for tok in tokens:\n","        seg_ids.append(current_seg_id)\n","        if tok == \"[SEP]\":\n","            current_seg_id = 1-current_seg_id # turns 1 into 0 and vice versa\n","    return seg_ids"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x06fFPFtFqVK"},"source":["We will create padded batches (so we pad sentences for each batch independently), this way we add the minimum of padding tokens possible. For that, we sort sentences by length, apply padded_batches and then shuffle."]},{"cell_type":"code","metadata":{"id":"hjAVGCwlb6F8"},"source":["data_with_len = [[sent, data_labels[i], len(sent)]\n","                 for i, sent in enumerate(data_inputs)]\n","random.shuffle(data_with_len)\n","data_with_len.sort(key=lambda x: x[2])\n","sorted_all = [([get_ids(sent_lab[0]),\n","                get_mask(sent_lab[0]),\n","                get_segments(sent_lab[0])],\n","               sent_lab[1])\n","              for sent_lab in data_with_len if sent_lab[2] > 4]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G16xVt98bED_"},"source":["sent_with_len = [[sent, sent_labels[i], len(sent)]\n","                 for i, sent in enumerate(data_inputs)]\n","random.shuffle(sent_with_len)\n","sent_with_len.sort(key=lambda x: x[2])\n","sorted_sent = [([get_ids(sent_lab[0]),\n","                get_mask(sent_lab[0]),\n","                get_segments(sent_lab[0])],\n","               sent_lab[1])\n","              for sent_lab in sent_with_len if sent_lab[2] > 4]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3YwzuRMMbMsr"},"source":["resp_with_len = [[sent, resp_labels[i], len(sent)]\n","                 for i, sent in enumerate(data_inputs)]\n","random.shuffle(resp_with_len)\n","resp_with_len.sort(key=lambda x: x[2])\n","sorted_resp = [([get_ids(sent_lab[0]),\n","                get_mask(sent_lab[0]),\n","                get_segments(sent_lab[0])],\n","               sent_lab[1])\n","              for sent_lab in resp_with_len if sent_lab[2] > 0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZkMiqmzsfo6a"},"source":["# A list is a type of iterator so it can be used as generator for a dataset\n","all_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,\n","                                             output_types=(tf.int32, tf.int32))\n","# A list is a type of iterator so it can be used as generator for a dataset\n","sent_dataset = tf.data.Dataset.from_generator(lambda: sorted_sent,\n","                                             output_types=(tf.int32, tf.int32))\n","# A list is a type of iterator so it can be used as generator for a dataset\n","resp_dataset = tf.data.Dataset.from_generator(lambda: sorted_resp,\n","                                             output_types=(tf.int32, tf.int32))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gkGWlzeOfos6"},"source":["BATCH_SIZE = 32\n","all_batched = all_dataset.padded_batch(BATCH_SIZE,\n","                                       padded_shapes=((3, None), ()),\n","                                       padding_values=(0, 0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5aA7it--hHl4"},"source":["NB_BATCHES = math.ceil(len(sorted_all) / BATCH_SIZE)\n","NB_BATCHES_TEST = NB_BATCHES // 10\n","all_batched.shuffle(NB_BATCHES)\n","test_dataset = all_batched.take(NB_BATCHES_TEST)\n","train_dataset = all_batched.skip(NB_BATCHES_TEST)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xtz0hLQKbwES"},"source":["sent_batched = sent_dataset.padded_batch(BATCH_SIZE,\n","                                       padded_shapes=((3, None), ()),\n","                                       padding_values=(0, 0))\n","test_sent = sent_batched.take(NB_BATCHES_TEST)\n","train_sent = sent_batched.skip(NB_BATCHES_TEST)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xxYnUHe5cDde"},"source":["resp_batched = resp_dataset.padded_batch(BATCH_SIZE,\n","                                       padded_shapes=((3, None), ()),\n","                                       padding_values=(0, 0))\n","test_resp = resp_batched.take(NB_BATCHES_TEST)\n","train_resp = resp_batched.skip(NB_BATCHES_TEST)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X4QCPok7aEM_"},"source":["next(iter(train_dataset))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N2pxAPFxGe8r"},"source":["# Stage 3: Model building"]},{"cell_type":"code","metadata":{"id":"L6DD3k3qPLDQ"},"source":["class DCNNBERTEmbedding(tf.keras.Model):\n","    \n","    def __init__(self,\n","                 nb_filters=50,\n","                 FFN_units=512,\n","                 nb_classes=2,\n","                 dropout_rate=0.1,\n","                 name=\"dcnn\"):\n","        super(DCNNBERTEmbedding, self).__init__(name=name)\n","        \n","        self.bert_layer = hub.KerasLayer(\n","            \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n","            trainable=False)\n","\n","        self.bigram = layers.Conv1D(filters=nb_filters,\n","                                    kernel_size=2,\n","                                    padding=\"valid\",\n","                                    activation=\"relu\")\n","        self.trigram = layers.Conv1D(filters=nb_filters,\n","                                     kernel_size=3,\n","                                     padding=\"valid\",\n","                                     activation=\"relu\")\n","        self.fourgram = layers.Conv1D(filters=nb_filters,\n","                                      kernel_size=4,\n","                                      padding=\"valid\",\n","                                      activation=\"relu\")\n","        self.pool = layers.GlobalMaxPool1D()\n","        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n","        self.dropout = layers.Dropout(rate=dropout_rate)\n","        if nb_classes == 2:\n","            self.last_dense = layers.Dense(units=1,\n","                                           activation=\"sigmoid\")\n","        else:\n","            self.last_dense = layers.Dense(units=nb_classes,\n","                                           activation=\"softmax\")\n","    \n","    def embed_with_bert(self, all_tokens):\n","        _, embs = self.bert_layer([all_tokens[:, 0, :],\n","                                   all_tokens[:, 1, :],\n","                                   all_tokens[:, 2, :]])\n","        return embs\n","\n","    def call(self, inputs, training):\n","        x = self.embed_with_bert(inputs)\n","\n","        x_1 = self.bigram(x)\n","        x_1 = self.pool(x_1)\n","        x_2 = self.trigram(x)\n","        x_2 = self.pool(x_2)\n","        x_3 = self.fourgram(x)\n","        x_3 = self.pool(x_3)\n","        \n","        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters)\n","        merged = self.dense_1(merged)\n","        merged = self.dropout(merged, training)\n","        output = self.last_dense(merged)\n","        \n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hsWpzQz2IQvJ"},"source":["# Stage 4: Training"]},{"cell_type":"code","metadata":{"id":"lhfUFvWEPOIf"},"source":["NB_FILTERS = 100\n","FFN_UNITS = 256\n","NB_CLASSES = 2\n","\n","DROPOUT_RATE = 0.2\n","\n","BATCH_SIZE = 32\n","NB_EPOCHS = 5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5HPbZ72KPPnX"},"source":["Dcnn = DCNNBERTEmbedding(nb_filters=NB_FILTERS,\n","                         FFN_units=FFN_UNITS,\n","                         nb_classes=NB_CLASSES,\n","                         dropout_rate=DROPOUT_RATE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JpHDseF0QLl3"},"source":["if NB_CLASSES == 2:\n","    Dcnn.compile(loss=\"binary_crossentropy\",\n","                 optimizer=\"adam\",\n","                 metrics=[\"accuracy\"])\n","else:\n","    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\n","                 optimizer=\"adam\",\n","                 metrics=[\"sparse_categorical_accuracy\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PHKv4Q6jcWeI"},"source":["DcnnSent = DCNNBERTEmbedding(nb_filters=NB_FILTERS,\n","                         FFN_units=FFN_UNITS,\n","                         nb_classes=3,\n","                         dropout_rate=DROPOUT_RATE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AYJ_Ld0wgjh-"},"source":["DcnnSent.compile(loss=\"sparse_categorical_crossentropy\",\n","                 optimizer=\"adam\",\n","                 metrics=[\"sparse_categorical_accuracy\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Dpx1QNFck9t"},"source":["DcnnResp = DCNNBERTEmbedding(nb_filters=NB_FILTERS,\n","                         FFN_units=FFN_UNITS,\n","                         nb_classes=NB_CLASSES,\n","                         dropout_rate=DROPOUT_RATE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TV7drp2Dg0pd"},"source":["DcnnResp.compile(loss=\"binary_crossentropy\",\n","                 optimizer=\"adam\",\n","                 metrics=[\"accuracy\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K1hdT_JT2Rfi"},"source":["checkpoint_path = \"./drive/My Drive/BERT/ckpt_bert_embedding/\"\n","\n","ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n","\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n","\n","# if ckpt_manager.latest_checkpoint:\n","#     ckpt.restore(ckpt_manager.latest_checkpoint)\n","#     print(\"Latest checkpoint restored!!\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e8LHztku2cjl"},"source":["class MyCustomCallback(tf.keras.callbacks.Callback):\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        ckpt_manager.save()\n","        print(\"\\nCheckpoint saved at {}.\".format(checkpoint_path))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V0C5lNxFTMrA"},"source":["## Result"]},{"cell_type":"code","metadata":{"id":"WrT8oWZzQNmW"},"source":["Dcnn.fit(train_dataset,\n","         epochs=NB_EPOCHS,\n","         callbacks=[MyCustomCallback()])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1sE7Ku4IcvxT"},"source":["DcnnSent.fit(train_sent,\n","         epochs=NB_EPOCHS)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"adQolwCdc3JV"},"source":["DcnnResp.fit(train_resp,\n","             epochs=NB_EPOCHS)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MAb_ijA5Idmz"},"source":["# Stage 5: Evaluation"]},{"cell_type":"code","metadata":{"id":"gQN-Y99WIf6m"},"source":["results = Dcnn.evaluate(test_dataset)\n","print(results)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lr49vQ3sdRXm"},"source":["results = DcnnSent.evaluate(test_sent)\n","print(results)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HKDpfR_ZdWM8"},"source":["results = DcnnResp.evaluate(test_resp)\n","print(results)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rj98dgxnmhak"},"source":["def get_prediction(sentence):\n","    tokens = encode_sentence(sentence)\n","\n","    input_ids = get_ids(tokens)\n","    input_mask = get_mask(tokens)\n","    segment_ids = get_segments(tokens)\n","\n","    inputs = tf.stack(\n","        [tf.cast(input_ids, dtype=tf.int32),\n","         tf.cast(input_mask, dtype=tf.int32),\n","         tf.cast(segment_ids, dtype=tf.int32)],\n","         axis=0)\n","    inputs = tf.expand_dims(inputs, 0) # simulates a batch\n","\n","    output = Dcnn(inputs, training=False)\n","\n","    sentiment = math.floor(output*2)\n","\n","    if sentiment == 0:\n","        print(\"Output of the model: {}\\nPredicted sentiment: negative\".format(\n","            output))\n","    elif sentiment == 1:\n","        print(\"Output of the model: {}\\nPredicted sentiment: positive\".format(\n","            output))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J9jC8UnJgOjS"},"source":["# get_prediction(\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0aMrBVRbeM29"},"source":[""],"execution_count":null,"outputs":[]}]}